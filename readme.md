# Error:0

---

[íŒ€ì†Œê°œ](https://www.notion.so/e15309d6534d41479ffa107871a452be)

       ì•ˆë…•í•˜ì„¸ìš”?  ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì„ êµ¬í˜„í•  ë•Œ ì—ëŸ¬ë¥¼ ì¤„ì´ëŠ”(error:0) ëœ»ìœ¼ë¡œ ê²°ì„±ëœ Error:0 íŒ€ì…ë‹ˆë‹¤. 

      ë˜í•œ ì €í¬ì¡°ëŠ” len(ì»´í“¨í„° ê³µí•™ì„ ì „ê³µí•œ ì‚¬ëŒ ìˆ˜)ë„ :0ì…ë‹ˆë‹¤.  ğŸ¥¸ğŸ˜ğŸ¤ª

     ë‹¤ì–‘í•œ ì „ê³µìë“¤ì´ ëª¨ì´ë‹¤ ë³´ë‹ˆ, ê°™ì€ ì£¼ì œ í•´ê²°ì„ ìœ„í•´ ê°ì ë‹¤ë¥¸ ì‹œê°ìœ¼ë¡œ ì ‘ê·¼í•  ìˆ˜ ìˆì—ˆê³ ,

ê·¸ ì¤‘ ì¢‹ì€ ì•„ì´ë””ì–´ì™€ ì¸ì‚¬ì´íŠ¸, ëª¨ë¸ë§ ê¸°ë²•ë“±ì„ ì„ ì •í•´ í•´ì»¤í†¤ì„ ì§„í–‰í–ˆìŠµë‹ˆë‹¤.

 ë¨¸ì‹ ëŸ¬ë‹, ë”¥ëŸ¬ë‹ ëª¨ë¸ì´ í•™ìŠµì„ ë°˜ë³µí• ìˆ˜ë¡ Errorê°€ ì¤„ì–´ë“œëŠ” ê²ƒì²˜ëŸ¼ ì €í¬ë„ ì•½ 2ì£¼ê°„ì˜ í•´ì»¤í†¤ 

ê¸°ê°„ë™ì•ˆ í•¨ê»˜ í•™ìŠµí•˜ë©° ë§ì€ ì‹œí–‰ì°©ì˜¤ ëì— ì—ëŸ¬ì½”ë“œë¥¼ ì¤„ì—¬ ëª¨ë¸ì„ ì™„ì„±í•  ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤. 

 ê·¸ ê²°ê³¼ë¬¼ì€ ì•„ë˜ì— ìˆìŠµë‹ˆë‹¤. ë§ì€ ê´€ì‹¬ ë¶€íƒë“œë¦½ë‹ˆë‹¤. ê°ì‚¬í•©ë‹ˆë‹¤.

```python
print(Error:0)
[ì´ìš´ì¬, ê³½ë¯¼ìˆ˜, ê¹€ë„ì€, ê¹€ëª…ìš°, ì´ìƒì¬]
```

---

[ì£¼ì œ](https://www.notion.so/bf6ee281b5d846ac9cd32f68b43fc956)

### [ì˜ì¹´ì˜ ì„±ê³µì ì¸ IPOë¥¼ ìœ„í•œ ì˜ì—…ì´ìµ ê°œì„  ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ ]

> ì†Œì£¼ì œ1 :  **ì˜ì¹´ì¡´ë³„ ìµœì  ì°¨ëŸ‰ ë°°ì¹˜ë¥¼ í†µí•œ ì´ìµ ê·¹ëŒ€í™” ë°©ì•ˆ**
                 ì†Œì£¼ì œ2 :  **ìš´ì˜ ì¢…ë£Œ ì˜ì¹´ì¡´ ì˜ˆì¸¡ìœ¼ë¡œ ì°¨ëŸ‰ ìš´ì˜ íš¨ìœ¨ í–¥ìƒ ë°©ì•ˆ**
> 

---

[ê°œìš”](https://www.notion.so/00c2ab8c7f53468ba283631f749f4e9f)

### ëª©ì°¨

1. [ë°ì´í„° ìˆ˜ì§‘ ë° ê¸°ìˆ  ì´ìš© ìŠ¤íƒ](https://www.notion.so/Error-0-1c255d1becf3468698ed989a34253a87) 
    1. ì˜ì¹´ ì œê³µ ë°ì´í„°
    2. ê¸°ìˆ  ì´ìš© ìŠ¤íƒ
2. [ë°ì´í„° ì „ì²˜ë¦¬](https://www.notion.so/Error-0-1c255d1becf3468698ed989a34253a87)
    1. Timeseries
    2. Resampling
    3. Lagging
3. [Train / Test Split](https://www.notion.so/Error-0-1c255d1becf3468698ed989a34253a87)
4. [ëª¨ë¸ë§](https://www.notion.so/Error-0-1c255d1becf3468698ed989a34253a87)
    1. GradientBoostingRegressor
    2. Prophet
    3. LSTM
5. [ëª¨ë¸ ê²°ê³¼ ë¶„ì„](https://www.notion.so/Error-0-1c255d1becf3468698ed989a34253a87)
    1. ëª¨ë¸ì„ ì •
    2. Clustering 
    3. Mapping
6. [í™œìš©ë°©ì•ˆ](https://www.notion.so/Error-0-1c255d1becf3468698ed989a34253a87)
    1. ì˜ì¹´ì¡´ë³„ ìµœì  ì°¨ëŸ‰ ë°°ì¹˜
    2. ì˜ì¹´ì¡´ ìš´ì˜ ì¢…ë£Œ ì¡°ê¸° ì˜ˆì¸¡
7. [ê¸°ëŒ€íš¨ê³¼](https://www.notion.so/Error-0-1c255d1becf3468698ed989a34253a87)
8. [íšŒê³ ](https://www.notion.so/Error-0-1c255d1becf3468698ed989a34253a87)

---

[ë‚´ìš©](https://www.notion.so/0026367a0a0c4787855f465f73f553fc)

1. **ë°ì´í„° ìˆ˜ì§‘ ë° ê¸°ìˆ  ì´ìš© ìŠ¤íƒ**
    1. ì˜ì¹´ ì œê³µ ë°ì´í„°
    ì €í¬ íŒ€ì€ ì˜ì¹´ì¸¡ì—ì„œ ê¸°ë³¸ìœ¼ë¡œ ì œê³µí•œ 2018ë…„ë„ 12ì›” 31ì¼ ë¶€í„° 2019ë…„ 11ì›” 29ì¼ê¹Œì§€ì˜ 79ë§Œê°œì˜ ë°ì´í„° ì…‹ì„ ìµœëŒ€í•œ ì´ìš©í•˜ì—¬ ìë£Œë¥¼ ë¶„ì„í–ˆìŠµë‹ˆë‹¤.
        - ì½”ë“œ ë³´ê¸°
            
            ```python
            timeseries_path = os.path.join(os.getcwd(), "drive", "MyDrive",  "new_socar_data.csv")
            timeseries = pd.read_csv(timeseries_path)
            timeseries.head()
            ```
            
            ![Untitled](Error%200%206b58920a14284e338b4ccff4d551f384/Untitled.png)
            
    2. ê¸°ìˆ  ì´ìš© ìŠ¤íƒ
        
        
        | ì–¸ì–´ | Python |
        | --- | --- |
        | AI / ML | TensorFlow , Scikit-Learn, Prophet |
        | ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ | Pandas, Numpy, matplotlib, tqdm |
        
2. **ë°ì´í„° ì „ì²˜ë¦¬**
    1. Zone Indexing
        - ì‹œê°„ ì œì•½ìœ¼ë¡œ ì „ì²´ ì˜ì¹´ì¡´ì´ ì•„ë‹Œ ì„œìš¸ì— ìˆëŠ” ì˜ì¹´ì¡´(ì•½440ì—¬ê°œ)ìœ¼ë¡œ í•œì •í–ˆìŠµë‹ˆë‹¤.
        - ì½”ë“œ ë³´ê¸°
            
            ```python
            candidates = list(timeseries["zone"].unique())
            nums = list(range(len(candidates)))
            table = {}
            for c, n in zip(candidates, nums):
              table[c] = n
            table
            timeseries.replace({"zone": table}, inplace=True)
            timeseries
            ```
            
            ![Untitled](Error%200%206b58920a14284e338b4ccff4d551f384/Untitled%201.png)
            
        
    2. ì‹œê³„ì—´ ë³€í™˜
        - Raw ë°ì´í„°ë¡œëŠ” ì•Œ ìˆ˜ ì—†ëŠ” ê°ê° ì˜ì¹´ì¡´ ë§ˆë‹¤ì˜ ì‹œê°„ëŒ€ë³„ ìˆ˜ìš”ëŸ‰ì„ ê³„ì‚°í•˜ê¸° ìœ„í•´ ì‹œê³„ì—´ ë°ì´í„° ë³€í™˜í–ˆìŠµë‹ˆë‹¤.
        - 2019ë…„ 1ì›” 1ì¼ë¶€í„° ë°ì´í„°ê°€ ì¡´ì¬í•˜ëŠ” 11ì›” 30ì¼ê¹Œì§€ë¥¼ 1ì‹œê°„ ë‹¨ìœ„ë¡œ ìª¼ê°œ ì‹œê°„ë³„ ì°¨ëŸ‰ ì‚¬ìš©í˜„í™©ì„ ì˜ì¹´ì¡´ë³„ë¡œ ì²˜ë¦¬í–ˆìŠµë‹ˆë‹¤. (440ê°œì¡´ x 8,016ì‹œê°„ = 352ë§Œê±´)
        - Raw ë°ì´í„° ì°¨ëŸ‰ ëŒ€ì—¬ì‹œì‘ì‹œê°„ê³¼ ë°˜ë‚©ì‹œê°„ ì •ë³´ë¥¼ ì´ìš©í•´ ëŒ€ì—¬ì‹œê°„ì •ë³´ë¥¼ ê³„ì‚°í•˜ê³ , ì°¨ëŸ‰ IDê°’ì„ ì´ìš©í•´ 1ì‹œê°„ë§ˆë‹¤ ì‹œê³„ì—´ ì…€ ì— ì ìš©í–ˆìŠµë‹ˆë‹¤.
        
         â€» ì „ì²´ ë°ì´í„°ë¥¼ ì‹œê³„ì—´ë¡œ ë³€í™˜í•˜ëŠ”ë° ì´ 45ì‹œê°„ ì •ë„ì˜ ë³€í™˜ì‹œê°„ ì†Œìš”ë©ë‹ˆë‹¤.
         (ì›”ë³„ ë³€í™˜ ì™„ë£Œì‹œ ì €ì¥í•´ Loss ìµœì†Œí™”)
        
        - ì½”ë“œë³´ê¸°
            
            ```python
            last_time = pd.to_datetime("2019-01-01 00:00:00")
            hour_added = datetime.timedelta(hours = 1)
            next_time = last_time + hour_added
            month = last_time.month
            cached = seoul_data[((seoul_data["reservation_start_at"] >= last_time) & (seoul_data["reservation_start_at"] < next_time)) | ((seoul_data["reservation_return_at"] >= last_time) & (seoul_data["reservation_return_at"] < next_time)) | ((seoul_data["reservation_start_at"] < last_time) & (seoul_data["reservation_return_at"] >= next_time))]
            for index, row in tqdm(zone_timeseries.iterrows()):
              if last_time > row["time"]:
                continue
              if month != row["time"].month:
                timeseries_month_path = os.path.join(os.getcwd(), "drive", "MyDrive", f"timeseries-m.csv")
                zone_timeseries[zone_timeseries["time"].apply(lambda x: x.month) == month].to_csv(timeseries_month_path)
                print(f"month {month} is saved...")
                month = row["time"].month
              if last_time != row["time"]:
                last_time = row["time"]
                next_time = last_time + hour_added
                cached = seoul_data[((seoul_data["reservation_start_at"] >= last_time) & (seoul_data["reservation_start_at"] < next_time)) | ((seoul_data["reservation_return_at"] >= last_time) & (seoul_data["reservation_return_at"] < next_time)) | ((seoul_data["reservation_start_at"] < last_time) & (seoul_data["reservation_return_at"] >= next_time))]
              filtered = cached[cached["zone_name"] == row["zone"]]
              cached = cached[cached["zone_name"] != row["zone"]]
              zone_timeseries.loc[index, "n_drive"] = filtered.shape[0]
              zone_timeseries.loc[index, "n_drive_unique"] = len(filtered["car_id"].unique())
            
            ```
            
            ![Untitled](Error%200%206b58920a14284e338b4ccff4d551f384/Untitled%202.png)
            
        
    3. Resampling
        - ì‹œê³„ì—´ ë°ì´í„°ë¡œ ë³€í™˜ëœ ì‹œê°„ë§ˆë‹¤ì˜ ì°¨ëŸ‰ ì‚¬ìš©í˜„í™©ì„ ê° ëª¨ë¸ì— ë§ê²Œ Resampling êµ¬í˜„í–ˆìŠµë‹ˆë‹¤.
        - ì½”ë“œë³´ê¸°
            
            ```python
            ## ì¡´ë³„ ê·¸ë£¹ë°”ì´ í•œ 
            timeseries["time"] = pd.to_datetime(timeseries["time"])
            ts=timeseries
            ts=ts.set_index('time')
            ts_resample= pd.DataFrame()
            ts_resample['n_drive_1Day'] = ts.groupby('zone').n_drive_unique.resample('1D').max()
            ts_resample.reset_index(inplace = True)
            ts_resample
            ```
            
            ![Untitled](Error%200%206b58920a14284e338b4ccff4d551f384/Untitled%203.png)
            
    4. Lagging
        - ì‹œê³„ì—´ ë°ì´í„°ë¥¼ ì´ìš©í•œ ì˜ˆì¸¡ëª¨ë¸ì„ ìƒì„±í•˜ê¸° ìœ„í•œ Lagging ì‘ì—…ì„ ì§„í–‰í–ˆìŠµë‹ˆë‹¤.
        - 1ì¼ ë‹¨ìœ„ë¡œ ë°ì´í„°ë¥¼ Resamplingí•˜ê³ , 50~56ë²ˆì˜ Laggingì„ í†µí•´ ì¼ìë³„ ìƒê´€ê´€ê³„ë¥¼ ì˜ˆì¸¡ ëª¨ë¸ì— ë°˜ì˜í•˜ê¸° ìœ„í•œ ì „ì²˜ë¦¬ ì‘ì—…ì„ ì‹œí–‰í–ˆìŠµë‹ˆë‹¤.
        - ì½”ë“œë³´ê¸°
            
            ```python
            for s in range(1,50):
              ts_resample22['shifted_{}'.format(s)] = ts_resample22.groupby('zone').n_drive_1day.shift(s)
            ts_resample22
            ```
            
            ![Untitled](Error%200%206b58920a14284e338b4ccff4d551f384/Untitled%204.png)
            
    
3. **Train / Test split**
    - ì‹œê³„ì—´ ë°ì´í„°ì˜ ë²”ìœ„ëŠ” 2019ë…„ 1ì›” 1ì¼ ~ 11ì›” 30ë¡œ **ì „ì²´ë¥¼ 9ê°œì›” : 2ê°œì›”ë¡œ Split** ì„ í–ˆìŠµë‹ˆë‹¤.
    - 1ì›”1ì¼ë¶€í„° 9ì›” 30ì¼ê¹Œì§€ë¥¼ Trainë°ì´í„°ë¡œ ì‚¬ìš©í•´ í•™ìŠµì„ ì‹œí–‰í–ˆìŠµë‹ˆë‹¤.
    - 10ì›”1ì¼~11ì›” 30ì¼ê¹Œì§€ Test ë°ì´í„°ë¡œ ì‚¬ìš©í•´ ì„±ëŠ¥ í™•ì¸ì„ ì‹œí–‰í–ˆìŠµë‹ˆë‹¤.
    - ì½”ë“œë³´ê¸°
        
        ```python
        train = ts_resamplse22.query("time <= '2019-09-30 23:00:00'")
        test = ts_resamplse22.query("time > '2019-09-30 23:00:00'")
        x_train = np.asarray(train.drop(['n_drive_1day', 'time'],1), dtype = np.float32)
        y_train = np.asarray(train[['n_drive_1day']], dtype = np.float32)
        x_test = np.asarray(test.drop(['n_drive_1day', 'time'],1), dtype = np.float32)
        y_test = np.asarray(test[['n_drive_1day']], dtype = np.float32)
        ```
        
4. **ëª¨ë¸ë§**
    1. GradientBoostingRegressor
        - ì½”ë“œë³´ê¸°
            
            ```python
            # DEVIDE = zoneë³„dataí–‰ ê°¯ìˆ˜
            DEVIDE = len(ts_sample)//len(set(ts_sample["zone"].unique()))
            MSE = list()
            split_num = round(DEVIDE*0.8)
            
            # zoneë³„ë¡œ GradientBoostingRegressorëª¨ë¸ ìƒì„œ í›„ MSE ì¸¡ì •
            for i in range(len(ts_sample)//DEVIDE):
              d_sample = ts_sample[DEVIDE*i:DEVIDE*(i+1)]
            
              train = d_sample[:split_num]
              test = d_sample[split_num:]
              x_train = np.asarray(train.drop(['n_drive_1day','time'],1))
              y_train = np.asarray(train['n_drive_1day'])
              x_test = np.asarray(test.drop(['n_drive_1day','time'],1))
              y_test = np.asarray(test['n_drive_1day'])
            	
              reg = ensemble.GradientBoostingRegressor(n_estimators=200,max_depth=4,min_samples_leaf=1,learning_rate=0.05)
              reg.fit(x_train, y_train)
            
              mse = mean_squared_error(y_test, np.round(reg.predict(x_test)))
              MSE.append(mse)
            
              globals()['model_{}'.format(i)] = reg
            
              if i % 50 == 0:
                print("zone{} , The mean squared error (MSE) on test set: {:.4f}".format(i,mse))
            
            print()
            print("MSE average : {:.4f}".format(sum(MSE)/len(MSE)))
            
            # ê° ëª¨ë¸ì„ pickleí˜•íƒœë¡œ í•œ íŒŒì¼ì— ì €ì¥
            import pickle
            filename = "/content/drive/MyDrive/Colab Notebooks/data/socar_timeseries_models.sav"
            modlist = list()
            for i in range(len(ts_sample)//DEVIDE):
              modlist.append(globals()['model_{}'.format(i)])
            s = pickle.dump(modlist, open(filename, 'wb'))
            
            # Visualization (ì‹œê°í™” ë° ì˜ˆì¸¡ê°’ ë¹„êµ)
            for z in range(2):
            # z = zone number
              print(f'â–¼ zone{z} ì‹¤ì œê°’,ì˜ˆì¸¡ê°’ ë¹„êµ')
              print()
              d_sample = ts_sample[DEVIDE*z:DEVIDE*(z+1)]
            
              train = d_sample[:split_num]
              test = d_sample[split_num:]
              x_train = np.asarray(train.drop(['n_drive_1day','time'],1))
              y_train = np.asarray(train['n_drive_1day'])
              x_test = np.asarray(test.drop(['n_drive_1day','time'],1))
              y_test = np.asarray(test['n_drive_1day'])
            
              plt.figure(figsize=(16,3))
              plt.grid(True)
              X = test["time"]
              Y1 = y_test
              Y2 = np.round(globals()['model_{}'.format(z)].predict(x_test))
              plt.plot(X, Y1)
              plt.plot(X, Y2,color='r') # red line = ì˜ˆì¸¡ê°’
              plt.show()
              print()
            ```
            
        
        - ëª¨ë¸ë§ ê²°ê³¼ : ì¼ìë³„ ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’ ë¹„êµ
            
            ![1641568939975.png](Error%200%206b58920a14284e338b4ccff4d551f384/1641568939975.png)
            
    2. Prophet
        - ì½”ë“œë³´ê¸°
            
            ```python
            period=1
            group_yhat=list()
            group_yhat_lower=[]
            group_yhat_upper=[]
            group_ds=[]
            
            y_reals = []
            y_preds = []
            zone = []
            
            index_zone=ts_resample.zone.unique()[:zone_number]
            ### ì„œìš¸ ì „ì²´ ì˜ì¹´ ì¡´ì—ì„œ ì‹œí–‰(n=438)
            
            for i in tqdm(index_zone): 
                sample = ts_resample[ts_resample.zone ==i].copy()
            #    sample.drop('zone',axis=1,inplace=True)
            #    print(sample)
            
                if Train :
                    sample['ds'] = pd.to_datetime(sample['ds'])
                    sample.index= sample['ds']
                    sample.columns = ['ds','y']
                    
                    answer = sample.last('60D')
                    train = sample[sample['ds'] < answer['ds'].iloc[0]]
                    y_real = answer['y'].sum()
                    y_reals.append(y_real)
                else :
                    sample['ds'] = pd.to_datetime(sample['ds'])
                    sample.columns = ['zone','ds','y']
                    test= sample
                    
                m = Prophet(changepoint_prior_scale=0.8,
            
                  yearly_seasonality=False,
                  weekly_seasonality=True,
                  daily_seasonality = True,
                 # seasonality_prior_scale = 0.2,
                  holidays=holidays)
                
                m.fit(test)
                
                # ì˜ˆì¸¡ê¸°ê°„(30ì¼ë¡œ set)
                future = m.make_future_dataframe(periods=30)    
                forecast = m.predict(future)
                new = forecast[['trend','ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail(period)
                new.append(new,ignore_index=True)
                group_yhat.append(new["yhat"])
                group_yhat_upper.append(new["yhat_upper"])
                group_yhat_lower.append(new["yhat_lower"])
                group_ds.append(new["ds"])
                y_pred = forecast.iloc[-30:, :].yhat.sum()
                y_preds.append(y_pred)
                zone.append(i)
            
                # ì‹œê°í™”ë¶€ë¶„
                fig1 = m.plot(forecast)
                plt.show()
            ```
            
        
        - ëª¨ë¸ë§ ê²°ê³¼ : ì˜ˆì¸¡ê°’(íŒŒë€ìƒ‰ ì‹¤ì„ )ê³¼ ì‹¤ì œ Data(ê²€ì€ ì )
            
            ![result.png](Error%200%206b58920a14284e338b4ccff4d551f384/%EA%B2%B0%EA%B3%BC.png)
            
        - ì „ì²´ ì¶”ì´ ë° ìš”ì¼ë³„, ì‹œê°„ëŒ€ë³„ ì˜í–¥ì„ ë¶„ì„í•´ ì˜ˆì¸¡ëª¨ë¸ì— ë°˜ì˜
            
            ![performance.png](Error%200%206b58920a14284e338b4ccff4d551f384/performance.png)
            
        
    3. LSTM
        - ì½”ë“œë³´ê¸°
            
            ```python
            my_LSTM_model = Sequential()
            my_LSTM_model.add(LSTM(units = 104, 
                                       return_sequences = True, 
                                       input_shape = (52,1), 
                                       activation = 'tanh'))
            my_LSTM_model.add(LSTM(units = 52, activation = 'tanh'))
            my_LSTM_model.add(Dense(units=2))
                
                # Compiling 
            my_LSTM_model.compile(optimizer = SGD(lr = 0.01, decay = 1e-7, 
                                                     momentum = 0.9, nesterov = False),
                                     loss = 'mean_squared_error')
                
                # Fitting to the training set 
            my_LSTM_model.fit(x_train, y_train, epochs = 30, batch_size = 150, verbose = 1, shuffle=False)
                
            LSTM_prediction = my_LSTM_model.predict(x_test)
            
            print(LSTM_prediction)
            
            ```
            
        - ëª¨ë¸ë§ ê²°ê³¼ : ì¼ìë³„ ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’ ë¹„êµ
            
            ![Untitled](Error%200%206b58920a14284e338b4ccff4d551f384/Untitled%205.png)
            
5. **ê²°ê³¼ ë¶„ì„**
    1. ëª¨ë¸ ì„ ì •
        - 3ê°€ì§€ ë°©ë²•ì˜ ëª¨ë¸ë§ ê¸°ë²•ì„ 440ì—¬ê°œì˜ ëª¨ë“  ì˜ì¹´ì¡´ì— ê°ê° Fittingí•´ MSE(Mean Square Error)ë¥¼ ê³„ì‚°í–ˆìŠµë‹ˆë‹¤.
        - ê·¸ ê²°ê³¼, ê°€ì¥ ì„±ëŠ¥ì´ ì¢‹ì€ GradientBoostingRegressor ëª¨ë¸ì„ ìµœì¢… ì„ íƒí–ˆìŠµë‹ˆë‹¤.
        
        â€» í‰ê·  MSEê°’ : **GradientBoosting : 1.08** /  **Prophet : 1.31** / LSTM : 1.38
        
        ![mse.png](Error%200%206b58920a14284e338b4ccff4d551f384/mse.png)
        
    2. *Clustering*
        - ì„ ì •í•œ ëª¨ë¸ì´ ì˜ˆì¸¡í•œ ì˜ì¹´ì¡´ì˜ ìˆ˜ìš”ë¥¼ í†µí•´ **ì˜ì¹´ ì¡´ë³„ ì„±ì¥ ê°€ëŠ¥ì„± Featureë¥¼** ì¶”ì¶œ í–ˆìŠµë‹ˆë‹¤.
        - K-Means Clustering ê¸°ë²•ì„ ì´ìš©í•´ ì „ì²´ Socarì¡´ì„ 4ê°œë¡œ êµ¬ë¶„í–ˆìŠµë‹ˆë‹¤.
        - ì½”ë“œë³´ê¸°
            
            ```python
            sns.lmplot(x='n_drive_avg', y='growth', data=Z, fit_reg=False,  # x-axis, y-axis, data, no line
                       scatter_kws={"s": 150}, # marker size
                       hue="cluster_id") # color
            # title
            title_font = {
                'fontsize': 16,
                'fontweight': 'bold'
            }
            plt.title('Socar Zone K-Means Clustering Result',fontdict=title_font, pad=20)
            ```
            
    
    ![clustering.png](Error%200%206b58920a14284e338b4ccff4d551f384/clustering.png)
    
    - 4ê°œë¡œ Clustering ëœ idë¥¼ í™œìš©í•´ Clustering ì¡´ë³„ íŠ¹ì§• ë¶„ì„í–ˆìŠµë‹ˆë‹¤.
        
        â€» Query, Value_counts(), ë“±ì„ ì‚¬ìš©í•´ ì¢…í•© ë¶„ì„í–ˆìŠµë‹ˆë‹¤.
        
        - Cluster0 : ë³´ìœ  ì°¨ëŸ‰ì˜ ìˆ˜ëŠ” ë§ì§€ ì•Šìœ¼ë‚˜, ì§€ì—­ë³„ë¡œ ìš”ì§€ì— ìœ„ì¹˜í•˜ê±°ë‚˜,
        
                              ì§€ì†ì ì¸ ê³ ì •ìˆ˜ìš”ë¡œ ì¸í•´ ê¾¸ì¤€í•œ ì„±ì¥ì„ ë³´ì—¬ì£¼ëŠ” ì¡´ 
        
              >>> **í¬ì¸íŠ¸ ì¡´**(Point Zone)
        
        - Cluster1 : ì°¨ëŸ‰ìˆ˜ìš” ë° ìš´í–‰ì´ ë§ê³  ì„±ì¥ ê°€ëŠ¥ì„±ë„ ë†’ì•„ ì•ìœ¼ë¡œ ì „ë§ì´ ê¸°ëŒ€ë˜ëŠ” ì¡´
            
            >>> **ìŠˆí¼ ì˜ì¹´ì¡´**(Super_Socar Zone)
            
        - Cluster2 : ì°¨ëŸ‰ìˆ˜ìš”ê°€ ì ê³  ì„±ì¥ ê°€ëŠ¥ì„±ë„ ë‚®ì•„ ìš´ì˜ì´ ì¢…ë£Œë ê²ƒìœ¼ë¡œ ì˜ˆì¸¡ë˜ëŠ” ì¡´
        
               >>> **ì—”ë“œ ì¡´**(End Zone)
        
        - Cluster3 : ê³„ì ˆ ë³„ ìˆ˜ìš”ê°€ íƒ„ë ¥ì¸ ê³³ìœ¼ë¡œ ì‹œì¦Œë³„ ê¸‰ê²©í•œ ì„±ì¥ ë˜ëŠ” ê°ì†Œë¥¼ ë³´ì—¬ì£¼ëŠ” ì¡´
        
               >>> **íƒ€ê²Ÿ ì¡´**(Target Zone)
        
    
    c. *Mapping*
    
    - Clusteringëœ ì˜ì¹´ ì¡´ë“¤ì„ ì§€ë„ìƒì— í‘œì‹œí•´ ì§€ë¦¬ì  íŠ¹ì§• íŒŒì•…í–ˆìŠµë‹ˆë‹¤.
    - ì½”ë“œë³´ê¸°
        
        ```python
        map_osm = folium.Map(location=[37.5, 127], zoom_start=12)
        
        for index, row in z_name_total.iterrows():
            location = (row['zone_lat'], row['zone_lng'])
         
            if row['cluster_id']==3:
              folium.Marker(location, popup=row['zone'],icon = folium.Icon(color='purple', icon='circle')).add_to(map_osm)
            if row['cluster_id']==2:
              folium.Marker(location, popup=row['zone'],icon = folium.Icon(color='gray', icon='circle')).add_to(map_osm)
            if row['cluster_id']==1:
              folium.Marker(location, popup=row['zone'],icon = folium.Icon(color='orange', icon='star')).add_to(map_osm)
            if row['cluster_id']==0:
              folium.Marker(location, popup=row['zone']).add_to(map_osm)
        
        map_osm
        ```
        
    
    ![mapping.png](Error%200%206b58920a14284e338b4ccff4d551f384/mapping.png)
    

1. **í™œìš©ë°©ì•ˆ**
    1. **ì˜ì¹´ì¡´ë³„ ìµœì  ì°¨ëŸ‰ ë°°ì¹˜**
        
        â€» Target Zoneì€ ê³„ì ˆë³„ ìˆ˜ìš”ê°€ íƒ„ë ¥ì ì´ë¯€ë¡œ ìˆ˜ìš”ê°€ ì—†ëŠ” ì‹œì¦Œì—ëŠ” Super_Socar Zone ìœ¼ë¡œ 
        
            ì°¨ëŸ‰ì„ ì´ë™í•˜ê³ , ìˆ˜ìš”ê°€ ë†’ì€ ì‹œê¸°ì—ëŠ” Targetì¡´ì— ì°¨ëŸ‰ì„ ì§‘ì¤‘í•˜ëŠ” ë°©ë²•ì´ íš¨ê³¼ì ì…ë‹ˆë‹¤.
        
        - ì§€ì—­ì˜ Target Zone ë˜ëŠ” Super_Socar Zone ê³¼ ë‹¤ë¥¸ ì˜ì¹´ì¡´ ì‚¬ì´ì˜ ìƒê´€ê´€ê³„(Correlation)ë¥¼ ê³„ì‚°í•´ ì¸ê·¼ ì§€ì—­ì—ì„œ **ìˆ˜ìš”ê°€ ë°˜ëŒ€ë¡œ ì›€ì§ì´ëŠ” ì¡´ë“¤**ì„ Grouping í–ˆìŠµë‹ˆë‹¤.
        
            â–¶ ìˆ˜ìš”ê°€ ë°˜ëŒ€ë¡œ ì›€ì§ì´ëŠ” ì¡´ë“¤ : íŠ¹ì • ì¡´ì˜ ì°¨ëŸ‰ ìˆ˜ìš”ê°€ ëŠ˜ì–´ë‚¨ì— ë”°ë¼ ìˆ˜ìš”ê°€ ë¹ ì§€ëŠ” ì¡´ ì…ë‹ˆë‹¤.
        
        - Groupingí•œ ì˜ì¹´ì¡´ë“¤ì˜ ìœ íœ´ì°¨ëŸ‰ Dataë¥¼ Timelineìœ¼ë¡œ ë¶„ì„í•´ íŠ¹ì • ì‹œì ì˜ ì¡´ë‹¹ ìœ íœ´ì°¨ëŸ‰ê³¼ ë¶€ì¡±ì°¨ëŸ‰ íŒë‹¨í–ˆìŠµë‹ˆë‹¤.
        - ìœ íœ´ì°¨ëŸ‰ì´ ë§ì€ ì¡´ì—ì„œ ì°¨ëŸ‰ì´ ë¶€ì¡±í•œ ì¡´ìœ¼ë¡œì˜ ì°¨ëŸ‰ ì¬ë°°ì¹˜ ì‹¤ì‹œí–ˆìŠµë‹ˆë‹¤.
        
        â€» **AJíŒŒí¬ ë…¼í˜„ì , eí¸í•œì„¸ìƒ 4ë‹¨ì§€ AJíŒŒí¬, í’ì‚°ë¹Œë”©** ì¡´ì˜ ìœ íœ´ì°¨ëŸ‰ ìˆ˜ íƒ€ì„ë¼ì¸ ë¶„ì„
        
        ![3d_corr.png](Error%200%206b58920a14284e338b4ccff4d551f384/3d_corr.png)
        
        â‡’ 5ì›”ì´ˆì˜ ê²½ìš° **eí¸í•œì„¸ìƒ 4ë‹¨ì§€ AJíŒŒí¬, í’ì‚°ë¹Œë”©**ì¡´ì€ ì°¨ëŸ‰ì´ ë¶€ì¡±í–ˆìœ¼ë‚˜, ì¸ê·¼ **AJíŒŒí¬ ë…¼í˜„ì ì˜ ê²½ìš° ì°¨ëŸ‰ì„ ì¶©ë¶„íˆ ë³´ìœ í•˜ê³  ìˆì—ˆìŠµë‹ˆë‹¤.** 
        
        â‡’ ëª¨ë¸ì´ ì˜ˆì¸¡í•œ ê²°ê³¼ 12ì›”ì˜ ê²½ìš° **eí¸í•œì„¸ìƒ 4ë‹¨ì§€ AJíŒŒí¬**ì˜ ì°¨ëŸ‰ì„ **í’ì‚°ë¹Œë”©** ë˜ëŠ” **AJíŒŒí¬ë¡œ ì¬ë°°ì¹˜ í•  í•„ìš”ê°€ ìˆë‹¤**ê³  ë³´ì…ë‹ˆë‹¤. 
        
    2. **ì˜ì¹´ì¡´ ìš´ì˜ ì¢…ë£Œ ì‹œì  ì˜ˆì¸¡**
        - ì°¨ëŸ‰ìˆ˜ê°€ ë§ì§€ ì•Šì€ Point Zoneê³¼ End Zoneì„ ëŒ€ìƒìœ¼ë¡œ Classification ëª¨ë¸ì„ ìƒì„±í•´ ì˜ì¹´ì¡´ì˜ ìš´í–‰ì¢…ë£Œ ì‹œì ì„ ì¡°ê¸° ì˜ˆì¸¡í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        - ì¡´ë³„ ë°ì´í„°ë¥¼ Timelineìœ¼ë¡œ ë¶„ì„í•´ ìˆ˜ìš”ê°€ ì¤„ì–´ë“¤ê²ƒìœ¼ë¡œ ì˜ˆìƒë˜ëŠ” ì‹œì ê³¼ ê·¸ íŠ¸ë Œë“œ ë¶„ì„ì„ í†µí•œ ì¡´ë³„ ìš´í–‰ ì¢…ë£Œ ì‹œì ì„ ì¡°ê¸° íŒë‹¨í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        - ì‹œì¦Œë³„/ì¼ë³„ priorityë¥¼ íŒë‹¨í•´ ìš´í–‰ì¬ê°œ / ìš´í–‰ì¢…ë£Œë¥¼ ê²°ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. 

1. **ê¸°ëŒ€íš¨ê³¼ (42.6ì–µì› ì¬ë¬´ ê°œì„  íš¨ê³¼)**
    1. ì˜ì¹´ì¡´ë³„ ìµœì  ì°¨ëŸ‰ ë°°ì¹˜ë¥¼ í†µí•œ ìˆ˜ìµ ê·¹ëŒ€í™”ë¡œ **38.4ì–µì› ì˜ì—…ì´ìµ ì¦ê°€**ë¥¼ ì˜ˆìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    
      â€» ê³„ì‚°ì‹ : ì¡´ë³„ ìœ íœ´ì°¨ëŸ‰ 0 ì¸ ì‹œê°„ í‰ê·  * ì‹œê°„ë‹¹ ì°¨ëŸ‰ ëŒ€ì—¬ê°€ê²© * ì „êµ­ ì˜ì¹´ ì¡´ìˆ˜
    
    > (120H * 8,000ì› * 4000) = 3,840,000,000ì›
    > 
    
    b. ì°¨ëŸ‰ ìš´ì˜ íš¨ìœ¨ í–¥ìƒìœ¼ë¡œ ì°¨ëŸ‰ ê³ ì •ë¹„ ë° ê°ê°€ìƒê° ë“± **4.2ì–µì› ë¹„ìš©ì ˆê°**ì„ ì˜ˆìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    
      â€» ê³„ì‚°ì‹ : íš¨ìœ¨í–¥ìƒ ê¸°ê°„ * ì˜ˆìƒ ìš´ì˜ì¢…ë£Œ ì¡´ ìˆ˜ * (ê°ê°€ìƒê°ë¹„+ê³ ì •ë¹„)
    
    > 2Month * 200(ì „êµ­ê¸°ì¤€) * (0.2ì–µ * 1/36+0.005ì–µ) = 4.2ì–µì›
    > 
    
    c. ì˜ì¹´ì¡´ì˜ ì´ìµ ì°½ì¶œë¡œ ì¸í•œ ì „ì²´ ë§¤ì¶œì•¡ì˜ ì¦ê°€ë¡œ ì¸í•œ ì´ìµë¥  1.5í”„ë¡œ ì¦ê°€
    
      â€» ê³„ì‚°ì‹ : ë‹¹í•´ ê¸°ì¤€ ì„œìš¸ ì˜ì¹´ì¡´ ì˜ì—… ì´ìµ / ë‹¹í•´ ì˜ì¹´ ë§¤ì¶œì•¡ * 100
    
    > 2019ë…„ ê¸°ì¤€ ì˜ì¹´ì¡´ ì´ìµë¥  = 200ì–µì› / 2400ì–µì› * 100 = ì•½ 8.3%
          ì˜ì¹´ì¡´ ìµœì  ì°¨ëŸ‰ ìš´ì˜ íš¨ìœ¨ í–¥ìƒ í›„ ì´ìµë¥  = 238.4ì–µì›(200ì–µì› + 38.4ì–µì›) / 2438.4 ì–µì›(2400ì–µì› + 38.4 ì–µì›) * 100 = ì•½ 9.8 %
    > 
    
2. **íšŒê³ **
    1. ì´ë²ˆ ìŠ¤í„°ë””ë¥¼ í•˜ë©´ì„œ ë°ì´í„°ì™€ ì‹œê°„ì´ ë§ì´ ë¶€ì¡±í•¨ì„ ëŠê»´ ì €í¬ íŒ€ëª…ì²˜ëŸ¼ ì—ëŸ¬ë¥¼ ì¤„ì´ëŠ” ë” ì •í™•í•œ ëª¨ë¸ì„ êµ¬í˜„í•˜ê³ ì í•˜ëŠ” ìš•êµ¬ê°€ ë” ìƒê²¼ìŠµë‹ˆë‹¤. 
    2. ì—¬ìœ ê°€ ìˆë‹¤ë©´ ì•„ë˜ì™€ ê°™ì€ ëª¨ë¸ì„ ë‹¤ë¤„ë³´ê³  ì‹¶ìŠµë‹ˆë‹¤.
        
        [Time Series From Scratchâ€Š-â€ŠExponentially Weighted Moving Averages (EWMA) Theory and Implementation](https://towardsdatascience.com/time-series-from-scratch-exponentially-weighted-moving-averages-ewma-theory-and-implementation-607661d574fe)
        
    

[ì°¸ê³ ìë£Œ](https://www.notion.so/e3d2d7df847e4ef2bf6fe6a4a0fb88c4)

[Prophet](https://facebook.github.io/prophet/)

[ì˜ì¹´ ì‹¤ì „ ë°ì´í„°ë¡œ ë°°ìš°ëŠ” AI ì—”ì§€ë‹ˆì–´ ìœ¡ì„± ë¶€íŠ¸ìº í”„](https://classlion.net/class/detail/43)

[](https://ratsgo.github.io/natural%20language%20processing/2017/03/09/rnnlstm/)

[ãˆœì˜ì¹´ ê¸°ì—…ì •ë³´ - ì—°ë´‰ 4,327ë§Œì› | ì¡ì½”ë¦¬ì•„](https://www.jobkorea.co.kr/company/16152121)
