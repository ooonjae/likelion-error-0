# Error:0

---

[íŒ€ì†Œê°œ](https://www.notion.so/e15309d6534d41479ffa107871a452be)

       ì•ˆë…•í•˜ì„¸ìš”?  ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì„ êµ¬í˜„í•  ë•Œ ì—ëŸ¬ë¥¼ ì¤„ì´ê³ ì(error:0) ê²°ì„±ëœ Error:0 íŒ€ì…ë‹ˆë‹¤. 

      ë˜í•œ ì €í¬ì¡°ëŠ” len(ì»´í“¨í„° ê³µí•™ì„ ì „ê³µí•œ ì‚¬ëŒ ìˆ˜)ë„ :0ì…ë‹ˆë‹¤.  ğŸ¥¸ğŸ˜ğŸ¤ª

     ë‹¤ì–‘í•œ ì „ê³µìë“¤ì´ ëª¨ì´ë‹¤ ë³´ë‹ˆ, ê°™ì€ ì£¼ì œ í•´ê²°ì„ ìœ„í•´ ê°ì ë‹¤ë¥¸ ì‹œê°ìœ¼ë¡œ ì ‘ê·¼í•  ìˆ˜ ìˆì—ˆê³ ,

ê·¸ ì¤‘ ì¢‹ì€ ì•„ì´ë””ì–´ì™€ ì¸ì‚¬ì´íŠ¸, ëª¨ë¸ë§ ê¸°ë²•ë“±ì„ ì„ ì •í•´ í•´ì»¤í†¤ì„ ì§„í–‰í–ˆìŠµë‹ˆë‹¤.

 ë¨¸ì‹ ëŸ¬ë‹, ë”¥ëŸ¬ë‹ ëª¨ë¸ì´ í•™ìŠµì„ ë°˜ë³µí• ìˆ˜ë¡ Errorê°€ ì¤„ì–´ë“œëŠ” ê²ƒì²˜ëŸ¼ ì €í¬ë„ ì•½ 2ì£¼ê°„ì˜ í•´ì»¤í†¤ 

ê¸°ê°„ë™ì•ˆ í•¨ê»˜ í•™ìŠµí•˜ë©° ë§ì€ ì‹œí–‰ì°©ì˜¤ ëì— ì—ëŸ¬ì½”ë“œë¥¼ ì¤„ì—¬ ëª¨ë¸ì„ ì™„ì„±í•  ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤. 

 ê·¸ ê²°ê³¼ë¬¼ì€ ì•„ë˜ì— ìˆìŠµë‹ˆë‹¤. ë§ì€ ê´€ì‹¬ ë¶€íƒë“œë¦½ë‹ˆë‹¤. ê°ì‚¬í•©ë‹ˆë‹¤.

```python
print(Error:0)
[ì´ìš´ì¬, ê³½ë¯¼ìˆ˜, ê¹€ë„ì€, ê¹€ëª…ìš°, ì´ìƒì¬]
```

---

[ì£¼ì œ](https://www.notion.so/bf6ee281b5d846ac9cd32f68b43fc956)

### [ì˜ì¹´ì˜ ì„±ê³µì ì¸ IPOë¥¼ ìœ„í•œ ì˜ì—…ì´ìµ ê°œì„  ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ ]

> ì†Œì£¼ì œ1 :  **ì˜ì¹´ì¡´ë³„ ìµœì  ì°¨ëŸ‰ ë°°ì¹˜ë¥¼ í†µí•œ ì´ìµ ê·¹ëŒ€í™” ë°©ì•ˆ**
                 ì†Œì£¼ì œ2 :  **ìš´ì˜ ì¢…ë£Œ ì˜ì¹´ì¡´ ì˜ˆì¸¡ìœ¼ë¡œ ì°¨ëŸ‰ ìš´ì˜ íš¨ìœ¨ í–¥ìƒ ë°©ì•ˆ**
> 

---

[ê°œìš”](https://www.notion.so/00c2ab8c7f53468ba283631f749f4e9f)

### ëª©ì°¨

1. [ë°ì´í„° ìˆ˜ì§‘ ë° ê¸°ìˆ  ì´ìš© ìŠ¤íƒ](https://www.notion.so/Error-0-1c255d1becf3468698ed989a34253a87) 
    1. ì˜ì¹´ ì œê³µ ë°ì´í„°
    2. ê¸°ìˆ  ì´ìš© ìŠ¤íƒ
2. [ë°ì´í„° ì „ì²˜ë¦¬](https://www.notion.so/Error-0-1c255d1becf3468698ed989a34253a87)
    1. Zone naming & indexing
    2. ì‹œê³„ì—´ ë³€í™˜(Timeseries)
    3. Resampling
    4. Lagging
3. [Train / Test Split](https://www.notion.so/Error-0-1c255d1becf3468698ed989a34253a87)
4. [ëª¨ë¸ë§](https://www.notion.so/Error-0-1c255d1becf3468698ed989a34253a87)
    1. GradientBoostingRegressor
    2. Prophet
    3. LSTM
5. [ëª¨ë¸ ê²°ê³¼ ë¶„ì„](https://www.notion.so/Error-0-1c255d1becf3468698ed989a34253a87)
    1. ëª¨ë¸ì„ ì •
    2. Clustering 
    3. Mapping
6. [í™œìš©ë°©ì•ˆ](https://www.notion.so/Error-0-1c255d1becf3468698ed989a34253a87)
    1. ì˜ì¹´ì¡´ë³„ ìµœì  ì°¨ëŸ‰ ë°°ì¹˜
    2. ì˜ì¹´ì¡´ ìš´ì˜ ì¢…ë£Œ ì¡°ê¸° ì˜ˆì¸¡
7. [ê¸°ëŒ€íš¨ê³¼](https://www.notion.so/Error-0-1c255d1becf3468698ed989a34253a87)
8. [íšŒê³ ](https://www.notion.so/Error-0-1c255d1becf3468698ed989a34253a87)

---

[ë‚´ìš©](https://www.notion.so/0026367a0a0c4787855f465f73f553fc)

1. **ë°ì´í„° ìˆ˜ì§‘ ë° ê¸°ìˆ  ì´ìš© ìŠ¤íƒ**
    1. ì˜ì¹´ ì œê³µ ë°ì´í„°
    ì €í¬ íŒ€ì€ ì˜ì¹´ì¸¡ì—ì„œ ê¸°ë³¸ìœ¼ë¡œ ì œê³µí•œ 2018ë…„ë„ 12ì›” 31ì¼ ë¶€í„° 2019ë…„ 11ì›” 29ì¼ê¹Œì§€ì˜ 79ë§Œê°œì˜ ë°ì´í„° ì…‹ì„ ìµœëŒ€í•œ ì´ìš©í•˜ì—¬ ìë£Œë¥¼ ë¶„ì„í–ˆìŠµë‹ˆë‹¤.
        
        (ì €í¬ê°€ í•´ë‹¹ raw ìë£Œë¥¼ ì „ì²˜ë¦¬í•œ íŒŒì¼(csv)ë“¤ì„ ê°–ê³  ìˆìŠµë‹ˆë‹¤. ì•„ë˜ ë§í¬ ì²¨ë¶€í•˜ê² ìŠµë‹ˆë‹¤.
        [https://drive.google.com/drive/folders/1pzGgFZt7qCsCiztEjoZvXJCJuRBntgCX?usp=sharing](https://drive.google.com/drive/folders/1pzGgFZt7qCsCiztEjoZvXJCJuRBntgCX?usp=sharing)
        
        í›„ì— ë°ì´í„° í™•ì¸ ì›í•˜ì‹œë©´ ê³µìœ í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤.)
        
        - ì½”ë“œ ë³´ê¸°
            
            ```python
            timeseries_path = os.path.join(os.getcwd(), "drive", "MyDrive",  "new_socar_data.csv")
            timeseries = pd.read_csv(timeseries_path)
            timeseries.head()
            ```
            
            ![Untitled](Error%200%206b58920a14284e338b4ccff4d551f384/Untitled.png)
            
    2. ê¸°ìˆ  ì´ìš© ìŠ¤íƒ
        
        
        | ì–¸ì–´ | Python |
        | --- | --- |
        | AI / ML | TensorFlow , Scikit-Learn, Prophet |
        | ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ | Pandas, Numpy, matplotlib, tqdm, folium, pickle, seaborn |
        | ì‹œê°í™” | Tableau |
        
2. **ë°ì´í„° ì „ì²˜ë¦¬**
    1. Zone naming & indexing
        - ì„œìš¸ì— ìˆëŠ” ì˜ì¹´ì¡´
        â‡’ ì „ì²´ ì¤‘ ê°€ì¥ ì¡´ì´ ë§ì´ ë¶„í¬ë˜ì–´ ìˆëŠ” êµ¬ì—­ ì¤‘ ì„œìš¸ì§€ì—­ìœ¼ë¡œ(ì•½440ì—¬ê°œ)ìœ¼ë¡œ í•œì •í–ˆìŠµë‹ˆë‹¤.
            
            ![Untitled](Error%200%206b58920a14284e338b4ccff4d551f384/Untitled%201.png)
            
            â‡’ ë˜í•œ, ì´ ì¡´ë“¤ ì¤‘ì—ì„œ ì˜ì¹´ì¸¡ì—ì„œ ë°›ì€ ë°ì´í„° ìƒì˜ ì¡´ë“¤ì„ ì•ìœ¼ë¡œ ì €í¬ê°€ ë‹¤ë£¨ëŠ”
            
            â€œì˜ì¹´ì¡´â€(ì´í•˜ ì˜ì¹´ì¡´)ì´ë¼ ì •ì˜í–ˆìŠµë‹ˆë‹¤. 
            
            â‡’ í¬ë¡¤ë§í•œ ì‹¤ì œ ì˜ì¹´ì¡´ì´ ê·¸ ë‹¹ì‹œì˜ ì˜ì¹´ì¡´ì˜ ìƒí™©ê³¼ ì•ˆë§ìœ¼ë©°, 
            ì˜ì¹´ì¡´ì´ ì•„ë‹Œ ì¡´ì—ì„œ ë” ë§ì€ ì˜ˆì•½ì´ ì´ë£¨ì–´ì§„ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆì—ˆê³ , 
            í¬ë¡¤ë§í•œ ì‹¤ì œ ì˜ì¹´ì¡´ë§Œ ì¶”ë¦¬ë©´ ë¨¸ì‹ ëŸ¬ë‹ì„ ì ìš©í•˜ê¸°ì— ë°ì´í„°ê°€ ë§¤ìš° ì ì–´ì§€ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.
            
        - ì˜ì¹´ì¡´ì˜ ì´ë¦„ì„ uniqueë¡œ ì„ ì •
        â‡’ ê°™ì€ ì˜ì¹´ì¡´ ì£¼ì†Œì¸ë° ì˜ì¹´ì¡´ ì´ë¦„ì´ ë‹¤ë¥¸ ê²½ìš°ê°€ ë°œìƒí•˜ê±°ë‚˜ ê°™ì€ ì˜ì¹´ì¡´ ì´ë¦„ì¸ë° ì˜ì¹´ì¡´ ì£¼ì†Œê°€ ë‹¤ë¥¸ ê²½ìš°ê°€ ë°œìƒí•˜ëŠ”ë° ì˜ì¹´ì¡´ ì´ë¦„ì„ uniqueë¡œ ì£¼ì—ˆì„ ì‹œì˜ Lossë¥¼ ìµœì†Œí™” í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.
            
            <ì´ë¦„ value_counts() í•œ ê°’>
            
            ![Untitled](Error%200%206b58920a14284e338b4ccff4d551f384/Untitled%202.png)
            
            ![Untitled](Error%200%206b58920a14284e338b4ccff4d551f384/Untitled%203.png)
            
        
        - ì½”ë“œ ë³´ê¸°
            
            ```python
            candidates = list(timeseries["zone"].unique())
            nums = list(range(len(candidates)))
            table = {}
            for c, n in zip(candidates, nums):
              table[c] = n
            table
            timeseries.replace({"zone": table}, inplace=True)
            timeseries
            ```
            
            ![Untitled](Error%200%206b58920a14284e338b4ccff4d551f384/Untitled%204.png)
            
        
    2. ì‹œê³„ì—´ ë³€í™˜(Timeseries)
        - Raw ë°ì´í„°ë¡œëŠ” ì•Œ ìˆ˜ ì—†ëŠ” ê°ê° ì˜ì¹´ì¡´ ë§ˆë‹¤ì˜ ì‹œê°„ëŒ€ë³„ ìˆ˜ìš”ëŸ‰ì„ ê³„ì‚°í•˜ê¸° ìœ„í•´ ì‹œê³„ì—´ ë°ì´í„°ë¡œ ë³€í™˜í–ˆìŠµë‹ˆë‹¤.
        - 2019ë…„ 1ì›” 1ì¼ë¶€í„° ë°ì´í„°ê°€ ì¡´ì¬í•˜ëŠ” 11ì›” 30ì¼ê¹Œì§€ë¥¼ 1ì‹œê°„ ë‹¨ìœ„ë¡œ ìª¼ê°œ ì‹œê°„ë³„ ì°¨ëŸ‰ ìš´í–‰í˜„í™©ì„ ì˜ì¹´ì¡´ë³„ë¡œ ì •ë¦¬í–ˆìŠµë‹ˆë‹¤. (440ê°œì¡´ x 8,016ì‹œê°„ = 352ë§Œê±´)
        - ì°¨ëŸ‰ ëŒ€ì—¬ ì‹œì‘ê³¼ ë°˜ë‚© ì‹œê°„ ì •ë³´ë¥¼ ì´ìš©í•´ ëŒ€ì—¬ ì‹œê°„ì„ ê³„ì‚°í•˜ê³ , ì°¨ëŸ‰ IDê°’ì„ ì´ìš©í•´ ì‹œê°„ë§ˆë‹¤ ì¡´ë³„ë¡œ ì‚¬ìš©ì¤‘ì¸ ì°¨ëŸ‰ ìˆ˜ë¥¼ ì•Œ ìˆ˜ ìˆë„ë¡ ì‹œê³„ì—´ ë³€í™˜ì— ì‚¬ìš© í–ˆìŠµë‹ˆë‹¤.
        
         
        
        - ì½”ë“œë³´ê¸°
            
            ```python
            last_time = pd.to_datetime("2019-01-01 00:00:00")
            hour_added = datetime.timedelta(hours = 1)
            next_time = last_time + hour_added
            month = last_time.month
            cached = seoul_data[((seoul_data["reservation_start_at"] >= last_time) & (seoul_data["reservation_start_at"] < next_time)) | ((seoul_data["reservation_return_at"] >= last_time) & (seoul_data["reservation_return_at"] < next_time)) | ((seoul_data["reservation_start_at"] < last_time) & (seoul_data["reservation_return_at"] >= next_time))]
            for index, row in tqdm(zone_timeseries.iterrows()):
              if last_time > row["time"]:
                continue
              if month != row["time"].month:
                timeseries_month_path = os.path.join(os.getcwd(), "drive", "MyDrive", f"timeseries-m.csv")
                zone_timeseries[zone_timeseries["time"].apply(lambda x: x.month) == month].to_csv(timeseries_month_path)
                print(f"month {month} is saved...")
                month = row["time"].month
              if last_time != row["time"]:
                last_time = row["time"]
                next_time = last_time + hour_added
                cached = seoul_data[((seoul_data["reservation_start_at"] >= last_time) & (seoul_data["reservation_start_at"] < next_time)) | ((seoul_data["reservation_return_at"] >= last_time) & (seoul_data["reservation_return_at"] < next_time)) | ((seoul_data["reservation_start_at"] < last_time) & (seoul_data["reservation_return_at"] >= next_time))]
              filtered = cached[cached["zone_name"] == row["zone"]]
              cached = cached[cached["zone_name"] != row["zone"]]
              zone_timeseries.loc[index, "n_drive"] = filtered.shape[0]
              zone_timeseries.loc[index, "n_drive_unique"] = len(filtered["car_id"].unique())
            
            ```
            
            ![Untitled](Error%200%206b58920a14284e338b4ccff4d551f384/Untitled%205.png)
            
        
    3. Resampling
        - ì‹œê³„ì—´ ë°ì´í„°ë¡œ ë³€í™˜ëœ ì‹œê°„ë§ˆë‹¤ì˜ ì°¨ëŸ‰ ì‚¬ìš©í˜„í™©ì„ ì¼ë³„ & ì¡´ë³„ ìµœëŒ€ ì˜ˆì•½ ìš´í–‰ ì°¨ëŸ‰ ëŒ€ìˆ˜ë¥¼ êµ¬í•´ì„œ ê° ëª¨ë¸ì— ë§ê²Œ Resampling êµ¬í˜„í–ˆìŠµë‹ˆë‹¤.
        - ì½”ë“œë³´ê¸°
            
            ```python
            ## ì¡´ë³„ ê·¸ë£¹ë°”ì´ í•œ 
            timeseries["time"] = pd.to_datetime(timeseries["time"])
            ts=timeseries
            ts=ts.set_index('time')
            ts_resample= pd.DataFrame()
            ts_resample['n_drive_1Day'] = ts.groupby('zone').n_drive_unique.resample('1D').max()
            ts_resample.reset_index(inplace = True)
            ts_resample
            ```
            
            ![Untitled](Error%200%206b58920a14284e338b4ccff4d551f384/Untitled%206.png)
            
    4. Lagging
        - ì‹œê³„ì—´ ë°ì´í„°ë¥¼ ì´ìš©í•œ ì˜ˆì¸¡ëª¨ë¸ì„ ìƒì„±í•˜ê¸° ìœ„í•œ Lagging ì‘ì—…ì„ ì§„í–‰í–ˆìŠµë‹ˆë‹¤.
        - 1ì¼ ë‹¨ìœ„ë¡œ ë°ì´í„°ë¥¼ Resamplingí•˜ê³ , 50~56ë²ˆì˜ Laggingì„ í†µí•´ ì¼ë³„ & ì¡´ë³„ ìµœëŒ€ ì˜ˆì•½ ìš´í–‰ ì°¨ëŸ‰ ëŒ€ìˆ˜ ìƒê´€ê´€ê³„ë¥¼ ì˜ˆì¸¡ ëª¨ë¸ì— ë°˜ì˜í•˜ê¸° ìœ„í•´ ì‹œí–‰í–ˆìŠµë‹ˆë‹¤.
        - ì½”ë“œë³´ê¸°
            
            ```python
            for s in range(1,50):
              ts_resample22['shifted_{}'.format(s)] = ts_resample22.groupby('zone').n_drive_1day.shift(s+7)
            ts_resample22
            ```
            
            ![Untitled](Error%200%206b58920a14284e338b4ccff4d551f384/Untitled%207.png)
            
        
3. **Train / Test split**
    - ì‹œê³„ì—´ ë°ì´í„°ì˜ ë²”ìœ„ëŠ” 2019ë…„ 1ì›” 1ì¼ ~ 11ì›” 30ë¡œ ì „ì²´ë¥¼ 9ê°œì›” : 2ê°œì›”ë¡œ Split í–ˆìŠµë‹ˆë‹¤.
    - 1ì›”1ì¼ë¶€í„° 9ì›” 30ì¼ê¹Œì§€ë¥¼ Trainë°ì´í„°ë¡œ ì‚¬ìš©í•´ ëª¨ë¸ì— í•™ìŠµì„ ì‹œì¼°ìŠµë‹ˆë‹¤.
    - 10ì›”1ì¼~11ì›” 30ì¼ê¹Œì§€ Test ë°ì´í„°ë¡œ ì‚¬ìš©í•´ ì„±ëŠ¥ í™•ì¸ì„ í–ˆìŠµë‹ˆë‹¤.
        - ì½”ë“œë³´ê¸°
            
            ```python
            train = ts_resamplse22.query("time <= '2019-09-30 23:00:00'")
            test = ts_resamplse22.query("time > '2019-09-30 23:00:00'")
            x_train = np.asarray(train.drop(['n_drive_1day', 'time'],1), dtype = np.float32)
            y_train = np.asarray(train[['n_drive_1day']], dtype = np.float32)
            x_test = np.asarray(test.drop(['n_drive_1day', 'time'],1), dtype = np.float32)
            y_test = np.asarray(test[['n_drive_1day']], dtype = np.float32)
            ```
            
    
4. **ëª¨ë¸ë§**
    1. GradientBoostingRegressor
        - ì½”ë“œë³´ê¸°
            
            ```python
            # DEVIDE = zoneë³„dataí–‰ ê°¯ìˆ˜
            DEVIDE = len(ts_sample)//len(set(ts_sample["zone"].unique()))
            MSE = list()
            split_num = round(DEVIDE*0.8)
            
            # zoneë³„ë¡œ GradientBoostingRegressorëª¨ë¸ ìƒì„œ í›„ MSE ì¸¡ì •
            for i in range(len(ts_sample)//DEVIDE):
              d_sample = ts_sample[DEVIDE*i:DEVIDE*(i+1)]
            
              train = d_sample[:split_num]
              test = d_sample[split_num:]
              x_train = np.asarray(train.drop(['n_drive_1day','time'],1))
              y_train = np.asarray(train['n_drive_1day'])
              x_test = np.asarray(test.drop(['n_drive_1day','time'],1))
              y_test = np.asarray(test['n_drive_1day'])
            	
              reg = ensemble.GradientBoostingRegressor(n_estimators=200,max_depth=4,min_samples_leaf=1,learning_rate=0.05)
              reg.fit(x_train, y_train)
            
              mse = mean_squared_error(y_test, np.round(reg.predict(x_test)))
              MSE.append(mse)
            
              globals()['model_{}'.format(i)] = reg
            
              if i % 50 == 0:
                print("zone{} , The mean squared error (MSE) on test set: {:.4f}".format(i,mse))
            
            print()
            print("MSE average : {:.4f}".format(sum(MSE)/len(MSE)))
            
            # ê° ëª¨ë¸ì„ pickleí˜•íƒœë¡œ í•œ íŒŒì¼ì— ì €ì¥
            import pickle
            filename = "/content/drive/MyDrive/Colab Notebooks/data/socar_timeseries_models.sav"
            modlist = list()
            for i in range(len(ts_sample)//DEVIDE):
              modlist.append(globals()['model_{}'.format(i)])
            s = pickle.dump(modlist, open(filename, 'wb'))
            
            # Visualization (ì‹œê°í™” ë° ì˜ˆì¸¡ê°’ ë¹„êµ)
            for z in range(2):
            # z = zone number
              print(f'â–¼ zone{z} ì‹¤ì œê°’,ì˜ˆì¸¡ê°’ ë¹„êµ')
              print()
              d_sample = ts_sample[DEVIDE*z:DEVIDE*(z+1)]
            
              train = d_sample[:split_num]
              test = d_sample[split_num:]
              x_train = np.asarray(train.drop(['n_drive_1day','time'],1))
              y_train = np.asarray(train['n_drive_1day'])
              x_test = np.asarray(test.drop(['n_drive_1day','time'],1))
              y_test = np.asarray(test['n_drive_1day'])
            
              plt.figure(figsize=(16,3))
              plt.grid(True)
              X = test["time"]
              Y1 = y_test
              Y2 = np.round(globals()['model_{}'.format(z)].predict(x_test))
              plt.plot(X, Y1)
              plt.plot(X, Y2,color='r') # red line = ì˜ˆì¸¡ê°’
              plt.show()
              print()
            ```
            
        
        ëª¨ë¸ë§ ê²°ê³¼ : ì¼ìë³„ ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’ ë¹„êµ
        
        ![1641568939975.png](Error%200%206b58920a14284e338b4ccff4d551f384/1641568939975.png)
        
    2. Prophet
        - ì½”ë“œë³´ê¸°
            
            ```python
            period=1
            group_yhat=list()
            group_yhat_lower=[]
            group_yhat_upper=[]
            group_ds=[]
            
            y_reals = []
            y_preds = []
            zone = []
            
            index_zone=ts_resample.zone.unique()[:zone_number]
            ### ì„œìš¸ ì „ì²´ ì˜ì¹´ ì¡´ì—ì„œ ì‹œí–‰(n=438)
            
            for i in tqdm(index_zone): 
                sample = ts_resample[ts_resample.zone ==i].copy()
            #    sample.drop('zone',axis=1,inplace=True)
            #    print(sample)
            
                if Train :
                    sample['ds'] = pd.to_datetime(sample['ds'])
                    sample.index= sample['ds']
                    sample.columns = ['ds','y']
                    
                    answer = sample.last('60D')
                    train = sample[sample['ds'] < answer['ds'].iloc[0]]
                    y_real = answer['y'].sum()
                    y_reals.append(y_real)
                else :
                    sample['ds'] = pd.to_datetime(sample['ds'])
                    sample.columns = ['zone','ds','y']
                    test= sample
                    
                m = Prophet(changepoint_prior_scale=0.8,
            
                  yearly_seasonality=False,
                  weekly_seasonality=True,
                  daily_seasonality = True,
                 # seasonality_prior_scale = 0.2,
                  holidays=holidays)
                
                m.fit(test)
                
                # ì˜ˆì¸¡ê¸°ê°„(30ì¼ë¡œ set)
                future = m.make_future_dataframe(periods=30)    
                forecast = m.predict(future)
                new = forecast[['trend','ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail(period)
                new.append(new,ignore_index=True)
                group_yhat.append(new["yhat"])
                group_yhat_upper.append(new["yhat_upper"])
                group_yhat_lower.append(new["yhat_lower"])
                group_ds.append(new["ds"])
                y_pred = forecast.iloc[-30:, :].yhat.sum()
                y_preds.append(y_pred)
                zone.append(i)
            
                # ì‹œê°í™”ë¶€ë¶„
                fig1 = m.plot(forecast)
                plt.show()
            ```
            
        - ëª¨ë¸ë§ ê²°ê³¼ : ì˜ˆì¸¡ê°’(íŒŒë€ìƒ‰ ì‹¤ì„ )ê³¼ ì‹¤ì œ Data(ê²€ì€ ì )
            
            ![ê²°ê³¼.png](Error%200%206b58920a14284e338b4ccff4d551f384/%EA%B2%B0%EA%B3%BC.png)
            
        - ì „ì²´ ì¶”ì´ ë° ìš”ì¼ë³„, ì‹œê°„ëŒ€ë³„ ì˜í–¥ì„ ë¶„ì„í•´ ì˜ˆì¸¡ëª¨ë¸ì— ë°˜ì˜
            
            ![performance.png](Error%200%206b58920a14284e338b4ccff4d551f384/performance.png)
            
        
    3. LSTM
        - ì½”ë“œë³´ê¸°
            
            ```python
            #ì¼ë³„ ìµœëŒ€ ì°¨ëŸ‰ ìˆ˜ resampling
            timeseries_path = os.path.join(os.getcwd(), "drive", "MyDrive",  "timeseries_transformed.csv")
            timeseries = pd.read_csv(timeseries_path)
            
            timeseries["time"] = pd.to_datetime(timeseries["time"])
            ts=timeseries
            ts=ts.set_index('time')
            ts_resample= pd.DataFrame()
            ts_resample['n_drive_1Day'] = ts.groupby('zone').n_drive_unique.resample('1D').max()
            ts_resample.reset_index(inplace = True)
            ts_resample
            
            # resampling í•œ ë°ì´í„°ë“¤ì„ minmaxscaler ë¡œ ì •ê·œí™”
            minmax_scaler = MinMaxScaler()
            sc = minmax_scaler.fit_transform(ts_resample[['n_drive_1Day']])
            
            data = pd.DataFrame(sc, columns = ['n_drive_1day'], index= ts_resample.index)
            
            data
            
            # LSTM ëª¨ë¸ êµ¬í˜„
            my_LSTM_model = Sequential()
            my_LSTM_model.add(LSTM(units = 104, 
                                       return_sequences = True, 
                                       input_shape = (52,1), 
                                       activation = 'tanh'))
            my_LSTM_model.add(LSTM(units = 52, activation = 'tanh'))
            my_LSTM_model.add(Dense(units=2))
                
                # Compiling 
            my_LSTM_model.compile(optimizer = SGD(lr = 0.01, decay = 1e-7, 
                                                     momentum = 0.9, nesterov = False),
                                     loss = 'mean_squared_error')
                
                # Fitting to the training set 
            my_LSTM_model.fit(x_train, y_train, epochs = 30, batch_size = 150, verbose = 1, shuffle=False)
                
            LSTM_prediction = my_LSTM_model.predict(x_test)
            
            print(LSTM_prediction)
            
            LSTM_prediction1 = LSTM_prediction.mean(axis=1)
            LSTM_prediction1.shape
            LSTM_prediction1.reshape(26779,1)
            
            y_test2 = minmax_scaler.inverse_transform(y_test)
            LSTM_prediction2 = minmax_scaler.inverse_transform(LSTM_prediction)
            
            LSTM_prediction2
            LSTM_prediction1 = LSTM_prediction2.mean(axis=1)
            LSTM_prediction1.reshape(26779,1)
            
            mse = mean_squared_error(y_test2, LSTM_prediction1)
            mse
            
            plt.plot(y_test2, label='Origial')
            plt.plot(LSTM_prediction1, label='Prediction')
            plt.legend(loc=0)
            plt.title('Validation Results')
            plt.show()
            ```
            
        - ëª¨ë¸ë§ ê²°ê³¼ : ì¼ìë³„ ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’ ë¹„êµ
            
            ![Untitled](Error%200%206b58920a14284e338b4ccff4d551f384/Untitled%208.png)
            
5. **ê²°ê³¼ ë¶„ì„**
    1. ëª¨ë¸ ì„ ì •
        - 3ê°€ì§€ ë°©ë²•ì˜ ëª¨ë¸ë§ ê¸°ë²•ì„ 440ì—¬ê°œì˜ ëª¨ë“  ì˜ì¹´ì¡´ì— ê°ê° Fittingí•´ MSE(Mean Squared Error)ë¥¼ ê³„ì‚°í–ˆìŠµë‹ˆë‹¤.
        - ê·¸ ê²°ê³¼, ê°€ì¥ ì„±ëŠ¥ì´ ì¢‹ì€ GradientBoostingRegressor ëª¨ë¸ì„ ìµœì¢… ì„ íƒí–ˆìŠµë‹ˆë‹¤.
        
        â€» í‰ê·  MSEê°’ : **GradientBoosting : 1.08** /  **Prophet : 1.31** / LSTM : 1.38 (ë‹¨ìœ„ : ëŒ€ìˆ˜)
        
        ![mse.png](Error%200%206b58920a14284e338b4ccff4d551f384/mse.png)
        
    2. *Clustering*
        - ì„ ì •í•œ ëª¨ë¸ì´ ì˜ˆì¸¡í•œ ì˜ì¹´ì¡´ì˜ í–¥í›„ ìˆ˜ìš”ë¥¼ í†µí•´ **ì˜ì¹´ì¡´ ë³„ ì„±ì¥ ê°€ëŠ¥ì„± Featureë¥¼** ì¶”ì¶œ í–ˆìŠµë‹ˆë‹¤.
        - K-Means Clustering ê¸°ë²•ì„ ì´ìš©í•´ ì „ì²´ Socarì¡´ì„ 4ê°œë¡œ êµ¬ë¶„í–ˆìŠµë‹ˆë‹¤.
        - ì½”ë“œë³´ê¸°
            
            ![Untitled](Error%200%206b58920a14284e338b4ccff4d551f384/Untitled%209.png)
            
            ```python
            sns.lmplot(x='n_drive_avg', y='growth', data=Z, fit_reg=False,  # x-axis, y-axis, data, no line
                       scatter_kws={"s": 150}, # marker size
                       hue="cluster_id") # color
            # title
            title_font = {
                'fontsize': 16,
                'fontweight': 'bold'
            }
            plt.title('Socar Zone K-Means Clustering Result',fontdict=title_font, pad=20)
            ```
            
            ![clustering.png](Error%200%206b58920a14284e338b4ccff4d551f384/clustering.png)
            
    
    - 4ê°œë¡œ Clustering ëœ idë¥¼ í™œìš©í•´ ì¡´ë³„ íŠ¹ì§•ì„ ë¶„ì„í–ˆìŠµë‹ˆë‹¤.
        
        â€» Query, Value_counts(), groupby ë“±ì„ ì‚¬ìš©í•´ ì¢…í•© ë¶„ì„í–ˆìŠµë‹ˆë‹¤.
        
        - **í¬ì¸íŠ¸ ì¡´**(**Point Zone,** Cluster_0)
            
            = ë³´ìœ  ì°¨ëŸ‰ì˜ ìˆ˜ëŠ” ë§ì§€ ì•Šìœ¼ë‚˜, ì§€ì—­ë³„ë¡œ ìš”ì§€ì— ìœ„ì¹˜í•˜ê±°ë‚˜,
            ì§€ì†ì ì¸ ê³ ì •ìˆ˜ìš”ë¡œ ì¸í•´ ê¾¸ì¤€í•œ ì„±ì¥ì„ ë³´ì—¬ì£¼ëŠ” ì¡´ 
            
        - **ìŠˆí¼ ì˜ì¹´ì¡´**(**Super_Socar Zone,** Cluster_1)
            
            = ì°¨ëŸ‰ìˆ˜ìš” ë° ìš´í–‰ì´ ë§ê³  ì„±ì¥ ê°€ëŠ¥ì„±ë„ ë†’ì•„ ì•ìœ¼ë¡œ ì „ë§ì´ ê¸°ëŒ€ë˜ëŠ” ì¡´
            
        - **ì—”ë“œ ì¡´**(**End Zone,** Cluster_2)
            
            = ì°¨ëŸ‰ìˆ˜ìš”ê°€ ì ê³  ì„±ì¥ ê°€ëŠ¥ì„±ë„ ë‚®ì•„ ìš´ì˜ì´ ì¢…ë£Œë ê²ƒìœ¼ë¡œ ì˜ˆì¸¡ë˜ëŠ” ì¡´
            
        - **íƒ€ê²Ÿ ì¡´**(**Target Zone,** Cluster_3)
            
            = ê³„ì ˆ ë³„ ìˆ˜ìš”ê°€ íƒ„ë ¥ì¸ ê³³ìœ¼ë¡œ ì‹œì¦Œë³„ ê¸‰ê²©í•œ ì„±ì¥ ë˜ëŠ” ê°ì†Œë¥¼ ë³´ì—¬ì£¼ëŠ” ì¡´
            
        
               ****
        
    
    c. *Mapping*
    
    - Clusteringëœ ì˜ì¹´ ì¡´ë“¤ì„ ì§€ë„ìƒì— í‘œì‹œí•´ ì§€ë¦¬ì  íŠ¹ì§•ì„ íŒŒì•…í–ˆìŠµë‹ˆë‹¤.
    - ì½”ë“œë³´ê¸°
        
        ```python
        map_osm = folium.Map(location=[37.5, 127], zoom_start=12)
        
        for index, row in z_name_total.iterrows():
            location = (row['zone_lat'], row['zone_lng'])
         
            if row['cluster_id']==3:
              folium.Marker(location, popup=row['zone'],icon = folium.Icon(color='purple', icon='circle')).add_to(map_osm)
            if row['cluster_id']==2:
              folium.Marker(location, popup=row['zone'],icon = folium.Icon(color='gray', icon='circle')).add_to(map_osm)
            if row['cluster_id']==1:
              folium.Marker(location, popup=row['zone'],icon = folium.Icon(color='orange', icon='star')).add_to(map_osm)
            if row['cluster_id']==0:
              folium.Marker(location, popup=row['zone']).add_to(map_osm)
        
        map_osm
        ```
        
    
    ![mapping.png](Error%200%206b58920a14284e338b4ccff4d551f384/mapping.png)
    
6. **í™œìš©ë°©ì•ˆ**
    1. **ì˜ì¹´ì¡´ë³„ ìµœì  ì°¨ëŸ‰ ë°°ì¹˜**
        
        â€» Target Zoneì€ ê³„ì ˆë³„ ìˆ˜ìš”ê°€ íƒ„ë ¥ì ì´ë¯€ë¡œ ìˆ˜ìš”ê°€ ì—†ëŠ” ì‹œì¦Œì—ëŠ” Super_Socar Zone ìœ¼ë¡œ
            ì°¨ëŸ‰ì„ ì´ë™í•˜ê³ , ìˆ˜ìš”ê°€ ë†’ì€ ì‹œê¸°ëŠ” Targetì¡´ì— ì°¨ëŸ‰ì„ ì§‘ì¤‘í•˜ëŠ” ë°©ë²•ì´ íš¨ê³¼ì ì…ë‹ˆë‹¤. 
        
        - ì§€ì—­ì˜ Target ë˜ëŠ” Super_Socar Zone ê³¼ ë‹¤ë¥¸ ì˜ì¹´ì¡´ ì‚¬ì´ì˜ ìƒê´€ê´€ê³„(Correlation)ë¥¼ ê³„ì‚°í•´ ì¸ê·¼ ì§€ì—­ì—ì„œ ìˆ˜ìš”ê°€ ë°˜ëŒ€ë¡œ ì›€ì§ì´ëŠ” ì¡´ë“¤ì„ Grouping í–ˆìŠµë‹ˆë‹¤.
        
          â–¶ ìˆ˜ìš”ê°€ ë°˜ëŒ€ë¡œ ì›€ì§ì´ëŠ” ì¡´ : íŠ¹ì • ì¡´ì˜ ì°¨ëŸ‰ ìˆ˜ìš”ê°€ ëŠ˜ì–´ë‚¨ì— ë”°ë¼ ìˆ˜ìš”ê°€ ë–¨ì–´ì§€ëŠ” ì¡´
        
        ![Untitled](Error%200%206b58920a14284e338b4ccff4d551f384/Untitled%2010.png)
        
        - Groupingí•œ ì˜ì¹´ì¡´ë“¤ì˜ ìœ íœ´ì°¨ëŸ‰ Dataë¥¼ Timelineìœ¼ë¡œ ë¶„ì„í•´ íŠ¹ì • ì‹œì ì˜ ì¡´ë‹¹ ìœ íœ´ì°¨ëŸ‰ê³¼ ë¶€ì¡±ì°¨ëŸ‰ì„ íŒë‹¨í–ˆìŠµë‹ˆë‹¤.
        - ìœ íœ´ì°¨ëŸ‰ì´ ë§ì€ ì¡´ì—ì„œ ì°¨ëŸ‰ì´ ë¶€ì¡±í•œ ì¡´ìœ¼ë¡œì˜ ì°¨ëŸ‰ ì¬ë°°ì¹˜ë¥¼ ì‹¤ì‹œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        - ë”°ë¼ì„œ ì´ëŸ° regression ê¸°ë²•ì„ í†µí•´ ìŠˆí¼ ì˜ì¹´ì¡´ê³¼ íƒ€ê²Ÿì¡´ ì‚¬ì´ì˜ ì‹œê°„ëŒ€ë³„ ë¹„ìš´í–‰ ì°¨ëŸ‰ì„ ì˜ˆì¸¡í•´ ë¹„ìš´í–‰ ì°¨ëŸ‰ì„ ì¤„ì—¬ ì¤„ ìˆ˜ ìˆëŠ” ëª¨ë¸ì„ ë§Œë“¤ì–´ ë³´ì•˜ìŠµë‹ˆë‹¤
        
        â€» **AJíŒŒí¬ ë…¼í˜„ì , eí¸í•œì„¸ìƒ 4ë‹¨ì§€ AJíŒŒí¬, í’ì‚°ë¹Œë”©** ì¡´ì˜ ìœ íœ´ì°¨ëŸ‰ ìˆ˜ íƒ€ì„ë¼ì¸ ë¶„ì„
        
        ![3d_corr.png](Error%200%206b58920a14284e338b4ccff4d551f384/3d_corr.png)
        
        â‡’ 5ì›”ì´ˆì˜ ê²½ìš° **eí¸í•œì„¸ìƒ 4ë‹¨ì§€ AJíŒŒí¬, í’ì‚°ë¹Œë”©**ì¡´ì€ ì°¨ëŸ‰ì´ ë¶€ì¡±í–ˆìœ¼ë‚˜, ì¸ê·¼ **AJíŒŒí¬ ë…¼í˜„ì ì˜ ê²½ìš° ì°¨ëŸ‰ì„ ì¶©ë¶„íˆ ë³´ìœ í•˜ê³  ìˆì—ˆìŠµë‹ˆë‹¤.** 
        
        â‡’ ëª¨ë¸ì´ ì˜ˆì¸¡í•œ ê²°ê³¼ 12ì›”ì˜ ê²½ìš° **eí¸í•œì„¸ìƒ 4ë‹¨ì§€ AJíŒŒí¬**ì˜ ì°¨ëŸ‰ì„ **í’ì‚°ë¹Œë”©** ë˜ëŠ” **AJíŒŒí¬ë¡œ ì¬ë°°ì¹˜ í•  í•„ìš”ê°€ ìˆë‹¤**ê³  ë³´ì…ë‹ˆë‹¤. 
        
    2. **ì˜ì¹´ì¡´ ìš´ì˜ ì¢…ë£Œ ì‹œì  ì˜ˆì¸¡**
        - ì°¨ëŸ‰ìˆ˜ê°€ ë§ì§€ ì•Šì€ Point Zoneê³¼ End Zoneì„ ëŒ€ìƒìœ¼ë¡œ Classification ëª¨ë¸ì„ ìƒì„±í•´ ì˜ì¹´ì¡´ì˜ ìš´í–‰ì¢…ë£Œ ì‹œì ì„ **ì•½ 95% ì •í™•ë„**ë¡œ ì¡°ê¸° ì˜ˆì¸¡í–ˆìŠµë‹ˆë‹¤.
            
            ```python
            from sklearn.ensemble import GradientBoostingClassifier
            
            gb1 = GradientBoostingClassifier(random_state=0, max_depth=2) # by deafult 3
            gb1.fit(x_train, y_train)
            
            print("Accuracy on training set: {:.3f}".format(gb1.score(x_train, y_train)))
            print("Accuracy on test set: {:.3f}".format(gb1.score(x_test, y_test)))
            ```
            
            ![Untitled](Error%200%206b58920a14284e338b4ccff4d551f384/Untitled%2011.png)
            
        - ì¡´ë³„ ë°ì´í„°ë¥¼ Timelineìœ¼ë¡œ ë¶„ì„í•´ ìˆ˜ìš”ê°€ ì¤„ì–´ë“¤ê²ƒìœ¼ë¡œ ì˜ˆìƒë˜ëŠ” ì‹œì ê³¼ ê·¸ íŠ¸ë Œë“œ ë¶„ì„ì„ í†µí•œ ì¡´ë³„ ìš´í–‰ ì¢…ë£Œ ì‹œì ì„ ì¡°ê¸° íŒë‹¨í•  ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤.
        - ê¸°ì¡´ ìš´í–‰ì¢…ë£Œë¡œ íŒë‹¨ëœ ì¡´ë“¤ê³¼ ë¹„êµí–ˆì„ë•Œ ì•½ **2ë‹¬ì •ë„ ë¹ ë¥¸ ì˜ˆì¸¡**ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.
        - ì‹œê³„ì—´ê³¼ ê¸°ì¡´ Rawë°ì´í„°ë¥¼ ì—°ê³„í•´ Classification ê²°ê³¼ë¥¼ ì‹œê°ì ìœ¼ë¡œ í‘œì‹œí–ˆìŠµë‹ˆë‹¤.
            
            â€» ìš´í–‰ì¢…ë£Œ ì˜ˆì¸¡ëª¨ë¸ ì‹œê°í™” (ì¡´ë³„ ì°¨ëŸ‰ ìš´í–‰ í˜„í™©ì„ ì›”ë³„ë¡œ ë¶„ì„, ìš´í–‰ì¼ìˆ˜ í‘œì‹œ) 
            
            ![closed.png](Error%200%206b58920a14284e338b4ccff4d551f384/closed.png)
            
        
7. **ê¸°ëŒ€íš¨ê³¼** 
    
    ìœ„ í™œìš©ë°©ì•ˆì„ í†µí•´ ì•½ **52.1ì–µì›ì˜ ì¬ë¬´ ê°œì„  íš¨ê³¼**ê°€ ìˆì„ ê²ƒìœ¼ë¡œ ì˜ˆìƒë©ë‹ˆë‹¤. 
    
    ì´ëŠ” 2019ë…„ íšŒê³„ ê¸°ì¤€ ì˜ì—…ì†ì‹¤ì•¡ì¸ 716ì–µ ëŒ€ë¹„ ì•½  **7.28%ê°€ ê°œì„ **ëœ ìˆ˜ì¹˜ì…ë‹ˆë‹¤. 
    
    
    1) ì˜ì¹´ì¡´ë³„ ìµœì  ì°¨ëŸ‰ ë°°ì¹˜ë¥¼ í†µí•œ ìˆ˜ìµ ê·¹ëŒ€í™”ë¡œ **31.9ì–µì› ì˜ì—…ì´ìµ ì¦ê°€**
    
      â€» ì‚°ì¶œê·¼ê±° : ì¡´ë³„ ìœ íœ´ì°¨ëŸ‰ 0 ì¸ ì‹œê°„ í‰ê· /ë…„ x ì‹œê°„ë‹¹ ì°¨ëŸ‰ ëŒ€ì—¬ê°€ê²© x ì „êµ­ ì˜ì¹´ ì¡´ìˆ˜
    
    > 99.64 x 8,000ì› x 4000 = 3,188,480,000ì›
    
    
    
    2) ì°¨ëŸ‰ ìš´ì˜ íš¨ìœ¨ í–¥ìƒìœ¼ë¡œ ì°¨ëŸ‰ ê³ ì •ë¹„ ë° ê°ê°€ìƒê° ë“± **4.2ì–µì› ë¹„ìš©ì ˆê°**
    
      â€» ì‚°ì¶œê·¼ê±° : íš¨ìœ¨í–¥ìƒ ê¸°ê°„ x ì˜ˆìƒ ìš´ì˜ì¢…ë£Œ ì¡´ ìˆ˜ x (ê°ê°€ìƒê°ë¹„+ê³ ì •ë¹„)
    
    > 2Month x 200(ì „êµ­ê¸°ì¤€) x (0.2ì–µ x 1/36+0.005ì–µ) = 4.2ì–µì›
    
    
    
    3) ìš´ì˜ì¢…ë£Œ ì¡´ ì¡°ê¸° ê²°ì •ì„ í†µí•œ ì°¨ëŸ‰ ìˆ˜ìµí–¥ìƒ ìœ¼ë¡œ **ì˜ì—…ì´ìµ 16ì–µì› ì¦ê°€**
    
      â€» ì‚°ì¶œê·¼ê±° : íš¨ìœ¨í–¥ìƒ ê¸°ê°„ x ì°¨ëŸ‰ ìš´í–‰ì‹œê°„ ì¦ê°€ë¶„/ì›” x ì°¨ëŸ‰ ëŒ€ì—¬ê°€ê²© x ì¢…ë£Œ ì¡´ ì°¨ëŸ‰ìˆ˜
    
      â€» ì‚°ì¶œê·¼ê±° : íš¨ìœ¨í–¥ìƒ ê¸°ê°„ x ì˜ˆìƒ ìš´ì˜ì¢…ë£Œ ì¡´ ìˆ˜ x (ê°ê°€ìƒê°ë¹„+ê³ ì •ë¹„)
    
    > 2Month x 50H/ì›” x 8,000ì›/H x 2,000ëŒ€ (ì „êµ­ê¸°ì¤€) = 16ì–µì›
    
    
8. **íšŒê³ **
    
    ì˜ì¹´ëŠ” ë§¤ë…„ ì„±ì¥ì¤‘ì´ë©°, 2021ë…„ 3ë¶„ê¸° í‘ì ì „í™˜í–ˆìŠµë‹ˆë‹¤. ì €í¬ê°€ ê°œë°œí•œ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì„ í†µí•´
    
    ì˜ì¹´ì˜  ì¬ë¬´ìƒí™©ì´ ë”ìš± ê°œì„ ë˜ì–´ IPOì—ì„œ ì¶©ë¶„í•œ ë°¸ë¥˜ì—ì´ì…˜ì„ ì¸ì •ë°›ê¸°ë¥¼ ë°”ëë‹ˆë‹¤. 
    
    ê·¸ ìˆœê°„ì„ ì™¸ë¶€ê°€ ì•„ë‹Œ ë‚´ë¶€ì—ì„œ í•¨ê»˜ í•  ìˆ˜ ìˆìœ¼ë©´ ì¢‹ê² ìŠµë‹ˆë‹¤. 
    
    ì´ë²ˆ í•´ì»¤í†¤ì„ ì§„í–‰í•˜ë©´ì„œ ë°ì´í„°ì™€ ì‹œê°„ì˜ ë¶€ì¡±í•¨ì„ ëŠê»´ ì €í¬ íŒ€ëª…ì²˜ëŸ¼ ì—ëŸ¬ë¥¼ ë” ì¤„ì¼ ìˆ˜
    
    ìˆëŠ” ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì„ êµ¬í˜„í•˜ê³ ì í•˜ëŠ” ìš•êµ¬ê°€ ìƒê²¼ìŠµë‹ˆë‹¤. ê¸°íšŒê°€ ëœë‹¤ë©´ ì•„ë˜ì˜ ì •ë³´ ë“±ë„
    
    ë°˜ì˜í•´ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ë” í–¥ìƒì‹œí‚¤ê³  ì‹¶ìŠµë‹ˆë‹¤.
    
    [Time Series From Scratchâ€Š-â€ŠExponentially Weighted Moving Averages (EWMA) Theory and Implementation](https://towardsdatascience.com/time-series-from-scratch-exponentially-weighted-moving-averages-ewma-theory-and-implementation-607661d574fe)
    
    P.S. ê·€ì¤‘í•œ ë°ì´í„°ë¥¼ ì œê³µí•´ì¤€ ì˜ì¹´ ë°ì´í„° ê·¸ë£¹ì¥ë‹˜ê³¼ ì§ì›ë¶„ë“¤, í›Œë¥­í•œ ê°•ì˜ì™€ ì»¤ë¦¬í˜ëŸ¼ì„
           í†µí•´ AIì—”ì§€ë‹ˆì–´ì˜ ê¸¸ì„ ì—´ì–´ì£¼ì‹  ê°•ì‚¬ë‹˜ê³¼ ë§¤ë‹ˆì €ë‹˜ê»˜ë„ ê°ì‚¬ë“œë¦½ë‹ˆë‹¤.
    

[ì°¸ê³ ìë£Œ](https://www.notion.so/e3d2d7df847e4ef2bf6fe6a4a0fb88c4)

[Prophet](https://facebook.github.io/prophet/)

[ì˜ì¹´ ì‹¤ì „ ë°ì´í„°ë¡œ ë°°ìš°ëŠ” AI ì—”ì§€ë‹ˆì–´ ìœ¡ì„± ë¶€íŠ¸ìº í”„](https://classlion.net/class/detail/43)

[](https://ratsgo.github.io/natural%20language%20processing/2017/03/09/rnnlstm/)

[Basic Feature Engineering With Time Series Data in Python - Machine Learning Mastery](https://machinelearningmastery.com/basic-feature-engineering-time-series-data-python/)

[ãˆœì˜ì¹´ ê¸°ì—…ì •ë³´ - ì—°ë´‰ 4,327ë§Œì› | ì¡ì½”ë¦¬ì•„](https://www.jobkorea.co.kr/company/16152121)

[ì˜ì¹´/ì—°ê²°ê°ì‚¬ë³´ê³ ì„œ/2021.04.02](https://dart.fss.or.kr/dsaf001/main.do?rcpNo=20210402000328)
